% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/t_learner.R
\name{t_learner}
\alias{t_learner}
\title{T-Learner for Causal Treatment Effect Estimation}
\usage{
t_learner(
  base_model = NULL,
  mode = "regression",
  data,
  recipe = NULL,
  treatment,
  tune_params = list(),
  resamples = NULL,
  grid = 20,
  metrics = NULL,
  optimize = FALSE,
  policy = FALSE,
  policy_method = NULL,
  bootstrap = FALSE,
  bootstrap_iters = 100,
  bootstrap_alpha = 0.05
)
}
\arguments{
\item{base_model}{Either a parsnip model specification object, or a character string
specifying the base learner. Supported strings are:
\itemize{
\item \code{"random_forest"}: Random forest (via ranger)
\item \code{"mars"}: Multivariate adaptive regression splines
\item \code{"xgb"}: XGBoost
\item \code{"glmnet"}: Regularized regression
}}

\item{mode}{Model type: \code{"regression"} or \code{"classification"}}

\item{data}{A data frame containing the training data.}

\item{recipe}{A \code{recipe} object (from \code{recipes} package) specifying preprocessing steps.
Must include the outcome and treatment variables.}

\item{treatment}{A string specifying the name of the treatment variable column in \code{data}.
This variable should be binary (0/1 or FALSE/TRUE)}

\item{tune_params}{A named list of hyperparameters for the base model. Values can be:
\itemize{
\item Fixed (e.g., \code{mtry = 3})
\item Tuning parameters (e.g., \code{mtry = tune()})
Only parameters valid for the selected model will be used. Defaults to empty list.
}}

\item{resamples}{An \code{rset} object (e.g., from \code{rsample::vfold_cv()}) for tuning.
Required if any parameters in \code{tune_params} use \code{tune()}.}

\item{grid}{Integer indicating number of grid points for tuning (passed to \code{tune_grid()}).
Defaults to 20.}

\item{metrics}{A \code{yardstick::metric_set()} of performance metrics for tuning.
If NULL, uses RMSE for regression or accuracy for classification.}

\item{optimize}{Logical. Whether to perform Bayesian optimization after initial
grid search when tuning parameters. Defaults to FALSE.}

\item{policy}{Logical. Whether to compute optimal treatment policy. Defaults to FALSE.}

\item{policy_method}{Policy learning method: \code{"greedy"} (threshold-based) or
\code{"tree"} (policy tree). Required if \code{policy = TRUE}.}

\item{bootstrap}{Logical. Whether to perform bootstrap for confidence intervals.
Defaults to FALSE.}

\item{bootstrap_iters}{Number of bootstrap iterations if \code{bootstrap = TRUE}.
Defaults to 100.}

\item{bootstrap_alpha}{Alpha level for confidence intervals. Defaults to 0.05.}
}
\value{
An object of class \code{"t_learner"} containing:
\item{base_model}{The parsnip model specification used for fitting}
\item{model_fit}{List of two fitted workflow objects:
\itemize{
\item \code{model_fit_1}: fitted model on treated units
\item \code{model_fit_0}: fitted model on control units
}}
\item{effect_measures}{List of estimated treatment effects including:
\itemize{
\item For regression: ITE, ATE, ATT, ATC
\item For classification: Additional RD, RR, OR, NNT, PNS, PN
}
}
\item{effect_measures_boots}{(Only if bootstrap=TRUE) List with bootstrap CIs for all effects,
each element contains estimate, lower, and upper bounds}
\item{modeling_results}{(Only if tuning performed) Tuning results and best parameters}
\item{policy_details}{(Only if policy=TRUE) Contains:
\itemize{
\item For greedy method: best_threshold, best_gain, policy_vector, gain_curve
\item For tree method: policy_tree_model, best_gain, policy_vector
}
}
}
\description{
Implements the T-learner approach for estimating heterogeneous treatment effects.
The T-learner fits two separate models: one for the treated group and one for the
control group. Individual treatment effects (ITEs) are estimated as the difference
between the predicted outcomes of these two models for each unit.

The function supports:
\itemize{
\item Multiple base learners (random forest, MARS, XGBoost, and glmnet)
\item Both regression and classification problems
\item Comprehensive effect measures (ATE, ATT, ATC, RR, OR, NNT, etc.)
\item Hyperparameter tuning via grid search or Bayesian optimization
\item Bootstrap confidence intervals for all effect measures
\item Custom preprocessing via recipes
\item Policy learning for treatment assignment (greedy threshold and policy tree methods)
}
}
\details{
The T-learner works by:
\enumerate{
\item Splitting the data into treated and control groups
\item Fitting one model on the treated data and another on the control data
\item Predicting potential outcomes for each unit using both models
\item Estimating treatment effects as the difference between the two predicted outcomes
}

The policy learning feature helps identify optimal treatment assignment rules:
\itemize{
\item Greedy threshold: Finds the treatment effect threshold that maximizes total gain
\item Policy tree: Learns a decision tree for treatment assignment based on covariates
}
}
\section{Effect Measures}{

For classification problems, the following additional effect measures are computed:
\describe{
\item{RD}{Risk Difference: P(Y=1|T=1) - P(Y=1|T=0)}
\item{RR}{Relative Risk: P(Y=1|T=1)/P(Y=1|T=0)}
\item{OR}{Odds Ratio: \link{P(Y=1|T=1)/P(Y=0|T=1)}/\link{P(Y=1|T=0)/P(Y=0|T=0)}}
\item{NNT}{Number Needed to Treat: 1/RD}
\item{PNS}{Probability of Necessity and Sufficiency}
\item{PN}{Probability of Necessity}
}
}

\section{Policy Learning}{

When \code{policy = TRUE}, the function computes optimal treatment assignment rules:
\describe{
\item{Greedy Threshold}{Finds the treatment effect threshold that maximizes total gain}
\item{Policy Tree}{Learns a decision tree for treatment assignment based on covariates}
}
}

\section{Model Details}{

For each supported model type:
\describe{
\item{Random Forest}{Uses \code{ranger} engine. Tunable parameters: mtry, trees, min_n}
\item{MARS}{Uses \code{earth} engine. Tunable parameters: num_terms, prod_degree, prune_method}
\item{XGBoost}{Uses \code{xgboost} engine. Tunable parameters: tree_depth, trees, learn_rate,
mtry, min_n, sample_size, loss_reduction}
\item{GLMNet}{Uses \code{glmnet} engine. Tunable parameters: penalty, mixture}
}
}

\examples{
\dontrun{
# Example 1: Basic usage with random forest
t_fit <- t_learner(
  base_model = "random_forest",
  data = data,
  recipe = rec,
  treatment = "treatment",
  mode = "classification"
)

# Example 2: With policy learning (greedy threshold)
t_fit_policy <- t_learner(
  base_model = "xgb",
  data = data,
  recipe = rec,
  treatment = "treatment",
  policy = TRUE,
  policy_method = "greedy"
)

# Example 3: With policy tree and bootstrap CIs
t_fit_full <- t_learner(
  base_model = "glmnet",
  data = data,
  recipe = rec,
  treatment = "treatment",
  policy = TRUE,
  policy_method = "tree",
  bootstrap = TRUE,
  bootstrap_iters = 200
)
}
}
\seealso{
\code{\link[=predict.causal_learner]{predict.causal_learner()}} for making predictions on new data
}
