% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/x_learner.R
\name{x_learner}
\alias{x_learner}
\title{X-Learner for Causal Inference}
\usage{
x_learner(
  base_model = NULL,
  mode = "regression",
  data,
  recipe = NULL,
  treatment,
  tune_params = list(),
  resamples = NULL,
  grid = 20,
  metrics = NULL,
  optimize = FALSE,
  bootstrap = FALSE,
  bootstrap_iters = 100,
  bootstrap_alpha = 0.05,
  stability = FALSE,
  policy = FALSE,
  policy_method = NULL
)
}
\arguments{
\item{base_model}{Character or parsnip model specification for the base outcome models.
Supported models: "xgb", "random_forest", "glmnet", "mars".}

\item{mode}{Character. Outcome type: "regression" or "classification".}

\item{data}{A data frame containing the treatment variable, outcome, and covariates.}

\item{recipe}{A \code{recipes::recipe} object specifying preprocessing steps for predictors.}

\item{treatment}{Character. Name of the binary treatment variable (0/1).}

\item{tune_params}{Named list of parameters for tuning the base model. Use \code{tune()} for parameters to tune.}

\item{resamples}{Resampling object (from \code{rsample}) used for tuning. Required if tuning parameters are provided.}

\item{grid}{Integer or data frame. Number of points in the tuning grid or a custom grid.}

\item{metrics}{Metric set from \code{yardstick} for evaluating model performance. Defaults to RMSE for regression or Accuracy for classification.}

\item{optimize}{Logical. Whether to perform Bayesian optimization after grid search (only applies to base model tuning).}

\item{bootstrap}{Logical. Whether to compute bootstrap estimates and confidence intervals for treatment effects.}

\item{bootstrap_iters}{Integer. Number of bootstrap iterations.}

\item{bootstrap_alpha}{Numeric. Significance level for bootstrap confidence intervals (default 0.05 for 95\% CI).}

\item{stability}{Logical. Whether to compute stability measures of predictions across bootstrap iterations.}

\item{policy}{Logical. Whether to compute a treatment assignment policy based on estimated CATEs.}

\item{policy_method}{Character. Policy method to use ("greedy" supported).}

\item{cate_model}{Character or parsnip model specification for the CATE models.
Currently defaults to a random forest.}

\item{propensity_model}{Character or parsnip model specification for the propensity score model.
Currently defaults to logistic regression.}
}
\value{
An object of class \code{x_learner} (inherits \code{causal_learner}) containing:
\itemize{
\item \code{base_model}: Original base model specifications for treated and control groups.
\item \code{treatment}: Name of the treatment variable.
\item \code{data}: Original data used for training.
\item \code{model_fit}: List containing first-stage and second-stage fitted models:
\itemize{
\item \code{st_1_m_1} and \code{st_1_m_0}: First-stage outcome models for treated and control groups.
\item \code{st_2_m_1} and \code{st_2_m_0}: Second-stage CATE models for pseudo-effects D1 and D0.
}
\item \code{effect_measures}: Core causal effect estimates:
\itemize{
\item \code{ITE}: Individual treatment effect.
\item \code{ATE}: Average Treatment Effect.
\item \code{ATT}: Average Treatment Effect on Treated.
\item \code{ATC}: Average Treatment Effect on Control.
\item For classification: RD, RR, OR, RR*, NNT, PNS, PN, and predicted probabilities (\code{y1}, \code{y0}).
}
\item \code{effect_measures_boots} (optional): Bootstrap estimates and confidence intervals for all causal effect measures.
\item \code{stability_measures} (optional): Unit- and model-level stability metrics for predictions across bootstrap iterations:
\itemize{
\item Unit-level SD, CV, 95\% quantiles, max-min range.
\item Mean and median pairwise Kendallâ€™s tau rank correlation across iterations.
\item SD of ATT and ATC across bootstrap iterations.
\item Correlation matrices of predictions across bootstrap iterations.
}
\item \code{modeling_results} (optional): Tuning results if tuning was performed.
\item \code{policy_details} (optional): Information about the estimated treatment assignment policy:
\itemize{
\item \code{best_threshold}: Optimal CATE threshold for treatment assignment.
\item \code{best_gain}: Maximum gain achieved by the policy.
\item \code{policy_vector}: Recommended treatment assignment for each unit.
\item \code{gain_curve}: ggplot2 object displaying gain vs threshold.
}
}
}
\description{
The X-Learner is a meta-algorithm for estimating Conditional Average Treatment Effects (CATE) in causal inference.
It works especially well for imbalanced treatment groups and can leverage machine learning models for both the
outcome and CATE estimation. This implementation supports regression and binary classification outcomes,
with optional bootstrapping, stability measures, parameter tuning, and policy evaluation.
}
\details{
\subsection{X-Learner Algorithm Steps:}{
\enumerate{
\item \strong{First-stage outcome modeling}: Fit separate models for treated and control groups.
\item \strong{Pseudo-effect computation}: Compute residuals (D1 for treated, D0 for control) based on first-stage predictions.
\item \strong{Second-stage CATE modeling}: Fit models to pseudo-effects D1 and D0 to estimate treatment effects conditional on covariates.
\item \strong{CATE aggregation}: Compute individual treatment effect estimates as a weighted combination using propensity scores.
\item \strong{Optional bootstrapping}: Estimate variability and confidence intervals of effect measures and stability of predictions.
\item \strong{Optional policy evaluation}: Compute optimal treatment assignment based on estimated CATEs.
}

This implementation supports both regression and binary classification outcomes, allows hyperparameter tuning (grid and Bayesian optimization),
and provides detailed diagnostics including stability and policy evaluation.
}
}
\examples{
\dontrun{
library(tidymodels)
library(recipes)

# Generate synthetic data
set.seed(123)
n <- 1000
X <- matrix(rnorm(n * 5), n, 5)
tau <- 0.3 * X[,1] + 0.5 * X[,2]^2
W <- rbinom(n, 1, plogis(0.2 * X[,1] + 0.2 * X[,3]))
Y <- 0.5 * X[,3] + tau * W + rnorm(n)
data <- as_tibble(X) \%>\% mutate(W = W, Y = Y)

# Recipe for preprocessing
rec <- recipe(Y ~ ., data = data) \%>\% step_normalize(all_numeric_predictors())

# Fit X-learner with random forest
xl_fit <- x_learner(
  base_model = "random_forest",
  cate_model = "random_forest",
  propensity_model = "glmnet",
  data = data,
  recipe = rec,
  treatment = "W",
  tune_params = list(mtry = tune(), trees = 100),
  resamples = vfold_cv(data, v = 5),
  grid = 10,
  bootstrap = TRUE,
  bootstrap_iters = 50,
  stability = TRUE,
  policy = TRUE,
  policy_method = "greedy"
)

# Examine effect estimates
xl_fit$effect_measures
xl_fit$effect_measures_boots
xl_fit$stability_measures
xl_fit$policy_details$gain_curve
}
}
